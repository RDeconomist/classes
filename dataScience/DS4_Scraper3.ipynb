{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "firstScraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlbb34ejpcHaOEdTf2YdSl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDeconomist/classes/blob/main/DS4_Scraper1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Science - web scraper 3**\r\n",
        "\r\n",
        "Aim of the file:\r\n",
        "\r\n",
        "1.   Scrape multiple sites with a similar URL.\r\n",
        "2.   Do this efficiently by running a loop over an array.\r\n",
        "3.   Collect togeter the results."
      ],
      "metadata": {
        "id": "MVfixlVBK5we"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# // 1.  Import packages that we need:\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "# ////////////////////////////////////////////////////////////////"
      ],
      "outputs": [],
      "metadata": {
        "id": "r-JSinApK49d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction: using a base URL and injecting a series of stock tickers into it."
      ],
      "metadata": {
        "id": "YH5WjG6IPVr7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# // Set the base URL: \r\n",
        "url_base = \"https://www.google.com/finance/quote/{}:LON\"\r\n",
        "\r\n",
        "# // Add an array of tickers, for major UK banks: \r\n",
        "tickers = ['LLOY', 'NWG', 'BARC', 'HSBA', 'STAN', 'VMUK']\r\n",
        "\r\n",
        "# // Create an empty array that we are going to fill:\r\n",
        "urls = np.empty(length, dtype='S50')\r\n",
        "\r\n",
        "# // Loop across this array:\r\n",
        "for t in tickers:\r\n",
        "   # // Put the particular ticker into the base URL \r\n",
        "   stockURL = url_base.format(t)\r\n",
        "   # // Find the index value of this particular ticker.\r\n",
        "   i = tickers.index(t)\r\n",
        "   # // Fill the empty url, at the given index value, with the full url for this ticker\r\n",
        "   urls[i] = stockURL\r\n",
        "\r\n",
        "# // Print out the urls that we have   \r\n",
        "urls"
      ],
      "outputs": [],
      "metadata": {
        "id": "Vh8EIq2_PVXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this in a full example:"
      ],
      "metadata": {
        "id": "f5wu9mFuSO2e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# // Set the base url:\r\n",
        "url_base = \"https://www.google.com/finance/quote/{}:LON\"\r\n",
        "\r\n",
        "# // Pick the letters that we want to inject into this url:\r\n",
        "tickers = ['LLOY', 'NWG', 'BARC', 'HSBA', 'STAN', 'VMUK']\r\n",
        "\r\n",
        "# // Begin a loop, dealing with this tickers one by one:\r\n",
        "for t in tickers:\r\n",
        "   \r\n",
        "   # // Return the index number of the thing we are working with:\r\n",
        "   s = tickers.index(t)\r\n",
        "   \r\n",
        "   # // Build the URL for this iteration of the loop:\r\n",
        "   URL = url_base.format(t)\r\n",
        "   \r\n",
        "   # // Request the html from the URL:\r\n",
        "   html = requests.get(URL)\r\n",
        "   \r\n",
        "   # // Get the soup of this page\r\n",
        "   soup = BeautifulSoup(html.content, 'html.parser')\r\n",
        "   \r\n",
        "   # // Now get what we want from the page: \r\n",
        "   name = soup.find_all(\"h1\")\r\n",
        "   price = soup.find_all(\"div\", class_=\"YMlKec fxKbKc\")\r\n",
        "   ticker = soup.find_all(\"div\", class_=\"COaKTb OTVmSe\")\r\n",
        "   change = soup.find_all(\"div\", class_=\"JwB6zf\")\r\n",
        "   \r\n",
        "   name = name[0].text\r\n",
        "   price = price[0].text\r\n",
        "   change = change[0].text\r\n",
        "   \r\n",
        "   # // Group together:\r\n",
        "   results = [t, name, price, change]\r\n",
        "   \r\n",
        "   # // Sense check: print out what we have on this point in the loop:\r\n",
        "   s\r\n",
        "   t\r\n",
        "   results"
      ],
      "outputs": [],
      "metadata": {
        "id": "YVCVVxY3SMqu"
      }
    }
  ]
}